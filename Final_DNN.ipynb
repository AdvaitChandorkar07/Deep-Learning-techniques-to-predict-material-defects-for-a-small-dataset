{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDfmkKi45S2h",
        "outputId": "67aa6e97-459e-4d21-8d69-9227382673b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-a54b6937157a>:26: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.metrics import MeanAbsoluteError\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.applications import VGG16, ResNet50, InceptionV3, Xception\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "!pip install keras-tuner\n",
        "from kerastuner.tuners import RandomSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z9DxQCC7wPT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "Dataset = pd.read_csv(\"/content/Materialdata.csv\")\n",
        "x_set=Dataset.iloc[:,1:-1].values\n",
        "y_set=Dataset.iloc[:,-1].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq5O3CWLCYXe"
      },
      "outputs": [],
      "source": [
        "# Define the number of components to keep after PCA\n",
        "n_components = 15\n",
        "\n",
        "# Create a PCA object with the specified number of components\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit the PCA model to the input data and transform the data\n",
        "X_pca = pca.fit_transform(x_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdXAd3T3hWsU"
      },
      "outputs": [],
      "source": [
        "X_norm = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
        "Y_norm = MinMaxScaler(feature_range=(-0.99, 0.99))\n",
        "y_set = y_set.reshape(-1,1)\n",
        "# Normalize input and target variables\n",
        "\n",
        "X_pca[:,:] = X_norm.fit_transform(X_pca[:,:])\n",
        "y_scaled = Y_norm.fit_transform(y_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKxNAbVETSRx"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X_pca, y_scaled, test_size=1/3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdg65oRu-kyz"
      },
      "source": [
        "DNN UNTRAINED WITH PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlz59AzYWYs6"
      },
      "outputs": [],
      "source": [
        "input_shape=(15,)\n",
        "Output_shape = 1\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(6, activation='tanh', input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(5, activation='tanh'),\n",
        "    tf.keras.layers.Dense(3, activation='tanh'),\n",
        "    tf.keras.layers.Dense(2, activation='tanh'),\n",
        "    tf.keras.layers.Dense(Output_shape , activation='linear')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mse',\n",
        "              metrics=['mean_absolute_percentage_error'])\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqaGYvu4bPF4"
      },
      "outputs": [],
      "source": [
        "history=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=1000, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih8oMP9mi01l"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['mean_absolute_error'], label='mean_absolute_error')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='val_mean_absolute_error')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBvPgdWQ_Lh5"
      },
      "source": [
        "DNN TRAINED MODEL WITH PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob2-984K9UUD"
      },
      "outputs": [],
      "source": [
        "# Define the Autoencoder architecture\n",
        "input_layer = Input(shape=(15,))\n",
        "encoded_layer1 = Dense(6, activation='tanh')(input_layer)\n",
        "encoded_layer2 = Dense(5, activation='tanh')(encoded_layer1)\n",
        "encoded_layer3 = Dense(4, activation='tanh')(encoded_layer2)\n",
        "encoded_layer4 = Dense(3, activation='tanh')(encoded_layer3)\n",
        "decoded_layer1 = Dense(4, activation='tanh')(encoded_layer4)\n",
        "decoded_layer2 = Dense(5, activation='tanh')(decoded_layer1)\n",
        "decoded_layer3 = Dense(6, activation='tanh')(decoded_layer2)\n",
        "decoded_layer4 = Dense(15, activation='linear')(decoded_layer3)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded_layer4)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse', metrics=[MeanAbsolutePercentageError(), 'mape'])\n",
        "history = autoencoder.fit(x_train, x_train, epochs=1000, batch_size=32, shuffle=True, validation_data=(x_test, x_test))\n",
        "\n",
        "# Create the Encoder model\n",
        "encoder = Model(input_layer, encoded_layer4)\n",
        "\n",
        "# Freeze the Encoder layers\n",
        "encoder.trainable = False\n",
        "\n",
        "# Create the pre-trained model\n",
        "pretrained_model = Model(input_layer, encoder(input_layer))\n",
        "\n",
        "# Create the final autoencoder\n",
        "input_layer_final = Input(shape=(15,))\n",
        "encoded_layer1_final = Dense(6, activation='tanh')(input_layer_final)\n",
        "encoded_layer2_final = Dense(5, activation='tanh')(encoded_layer1_final)\n",
        "encoded_layer3_final = Dense(4, activation='tanh')(encoded_layer2_final)\n",
        "encoded_layer4_final = Dense(3, activation='tanh')(encoded_layer3_final)\n",
        "decoded_layer1_final = Dense(4, activation='tanh')(encoded_layer4_final)\n",
        "decoded_layer2_final = Dense(5, activation='tanh')(decoded_layer1_final)\n",
        "decoded_layer3_final = Dense(6, activation='tanh')(decoded_layer2_final)\n",
        "decoded_layer4_final = Dense(15, activation='linear')(decoded_layer3_final)\n",
        "\n",
        "autoencoder_final = Model(input_layer_final, decoded_layer4_final)\n",
        "autoencoder_final.compile(optimizer='adam', loss='mse', metrics=['mape'])\n",
        "history_final = autoencoder_final.fit(x_train, x_train, epochs=1000, batch_size=32, shuffle=True, validation_data=(x_test, x_test))\n",
        "\n",
        "# Create the Encoder model for the final autoencoder\n",
        "encoder_final = Model(input_layer_final, encoded_layer4_final)\n",
        "\n",
        "# Freeze the final autoencoder Encoder layers\n",
        "encoder_final.trainable = False\n",
        "\n",
        "# Create the pre-trained model for the final autoencoder\n",
        "pretrained_model_final = Model(input_layer_final, encoder_final(input_layer_final))\n",
        "\n",
        "# Extract the hidden layer output of the final autoencoder\n",
        "hidden_layer_output_final = encoder_final.predict(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-sNxw249VWN"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['mean_absolute_error'], label='mean_absolute_error')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation_mean_absolute_error')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 0.35])\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU60VIaxI7Nd"
      },
      "outputs": [],
      "source": [
        "X_norm = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
        "Y_norm = MinMaxScaler(feature_range=(-0.99, 0.99))\n",
        "y_set = y_set.reshape(-1,1)\n",
        "\n",
        "x_set[:,:] = X_norm.fit_transform(x_set[:,:])\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_set, y_scaled, test_size=1/3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OEno0Su29Zvf"
      },
      "outputs": [],
      "source": [
        "# Define the first Autoencoder\n",
        "input_layer1 = Input(shape=(21,))\n",
        "encoded_layer1 = Dense(6, activation='tanh')(input_layer1)\n",
        "encoded_layer2 = Dense(5, activation='tanh')(encoded_layer1)\n",
        "encoded_layer3 = Dense(4, activation='tanh')(encoded_layer2)\n",
        "encoded_layer4 = Dense(3, activation='tanh')(encoded_layer3)\n",
        "decoded_layer1 = Dense(4, activation='tanh')(encoded_layer4)\n",
        "decoded_layer2 = Dense(5, activation='tanh')(decoded_layer1)\n",
        "decoded_layer3 = Dense(6, activation='tanh')(decoded_layer2)\n",
        "decoded_layer4 = Dense(21, activation='linear')(decoded_layer3)\n",
        "\n",
        "autoencoder1 = Model(input_layer1, decoded_layer4)\n",
        "\n",
        "autoencoder1.compile(optimizer='adam', loss='mse', metrics=[MeanAbsoluteError(), 'mae'])\n",
        "history1 = autoencoder1.fit(x_train1, x_train1, epochs=1000, batch_size=32, shuffle=True, validation_data=(x_test1, x_test1))\n",
        "\n",
        "# Create the first Encoder model\n",
        "encoder1 = Model(input_layer1, encoded_layer4)\n",
        "\n",
        "# Freeze the first Encoder layers\n",
        "encoder1.trainable = False\n",
        "\n",
        "# Create the first pre-trained model\n",
        "pretrained_model1 = Model(input_layer1, encoder1(input_layer1))\n",
        "\n",
        "# Define the second Autoencoder\n",
        "input_layer2 = Input(shape=(3,))\n",
        "encoded_layer5 = Dense(4, activation='tanh')(input_layer2)\n",
        "encoded_layer6 = Dense(5, activation='tanh')(encoded_layer5)\n",
        "encoded_layer7 = Dense(6, activation='tanh')(encoded_layer6)\n",
        "decoded_layer5 = Dense(5, activation='tanh')(encoded_layer7)\n",
        "decoded_layer6 = Dense(4, activation='tanh')(decoded_layer5)\n",
        "decoded_layer7 = Dense(3, activation='tanh')(decoded_layer6)\n",
        "decoded_layer8 = Dense(1, activation='linear')(decoded_layer7)\n",
        "\n",
        "autoencoder2 = Model(input_layer2, decoded_layer8)\n",
        "\n",
        "autoencoder2.compile(optimizer='adam', loss='mse', metrics=[MeanAbsoluteError(), 'mae'])\n",
        "history2 = autoencoder2.fit(encoder1.predict(x_train1), x_train1, epochs=1000, batch_size=32, shuffle=True, validation_data=(encoder1.predict(x_test1), x_test1))\n",
        "\n",
        "# Create the second Encoder model\n",
        "encoder2 = Model(input_layer2, encoded_layer7)\n",
        "\n",
        "# Freeze the second Encoder layers\n",
        "encoder2.trainable = False\n",
        "\n",
        "# Create the pre-trained model\n",
        "pretrained_model2 = Model(input_layer1, encoder2(encoder1(input_layer1)))\n",
        "\n",
        "# Combine the two Autoencoders\n",
        "autoencoder = Model(input_layer1, autoencoder2(encoder1(input_layer1)))\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse', metrics=[MeanAbsoluteError(), 'mae','mean_absolute_percentage_error'])\n",
        "history = autoencoder.fit(x_train1, x_train1, epochs=1000, batch_size=32, shuffle=True, validation_data=(x_test1, x_test1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-Z4aVKrNlIn"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['mean_absolute_error'], label='mean_absolute_error')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation_mean_absolute_error')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 0.35])\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He0d_h7V-Gno"
      },
      "outputs": [],
      "source": [
        "# Define the Autoencoder architecture\n",
        "input_layer = Input(shape=(21,))\n",
        "encoded_layer1 = Dense(6, activation='tanh')(input_layer)\n",
        "encoded_layer2 = Dense(5, activation='tanh')(encoded_layer1)\n",
        "encoded_layer3 = Dense(4, activation='tanh')(encoded_layer2)\n",
        "encoded_layer4 = Dense(3, activation='tanh')(encoded_layer3)\n",
        "decoded_layer1 = Dense(4, activation='tanh')(encoded_layer4)\n",
        "decoded_layer2 = Dense(5, activation='tanh')(decoded_layer1)\n",
        "decoded_layer3 = Dense(6, activation='tanh')(decoded_layer2)\n",
        "decoded_layer4 = Dense(21, activation='linear')(decoded_layer3)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded_layer4)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse', metrics=[MeanAbsoluteError(), 'mae'])\n",
        "history = autoencoder.fit(x_train1, x_train1, epochs=1000, batch_size=32, shuffle=True, validation_data=(x_test1, x_test1))\n",
        "\n",
        "# Create the Encoder model\n",
        "encoder = Model(input_layer, encoded_layer4)\n",
        "\n",
        "# Freeze the Encoder layers\n",
        "encoder.trainable = False\n",
        "\n",
        "# Create the pre-trained model\n",
        "pretrained_model = Model(input_layer, encoder(input_layer))\n",
        "\n",
        "# Create the final autoencoder\n",
        "input_layer_final = Input(shape=(21,))\n",
        "encoded_layer1_final = Dense(6, activation='tanh')(input_layer_final)\n",
        "encoded_layer2_final = Dense(5, activation='tanh')(encoded_layer1_final)\n",
        "encoded_layer3_final = Dense(4, activation='tanh')(encoded_layer2_final)\n",
        "decoded_layer1_final = Dense(4, activation='tanh')(encoded_layer4_final)\n",
        "encoded_layer4_final = Dense(3, activation='tanh')(encoded_layer3_final)\n",
        "decoded_layer2_final = Dense(5, activation='tanh')(decoded_layer1_final)\n",
        "decoded_layer3_final = Dense(6, activation='tanh')(decoded_layer2_final)\n",
        "decoded_layer4_final = Dense(21, activation='linear')(decoded_layer3_final)\n",
        "\n",
        "autoencoder_final = Model(input_layer_final, decoded_layer4_final)\n",
        "autoencoder_final.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "history_final = autoencoder_final.fit(x_train1, x_train1, epochs=1000, batch_size=32, shuffle=True, validation_data=(x_test1, x_test1))\n",
        "\n",
        "# Create the Encoder model for the final autoencoder\n",
        "encoder_final = Model(input_layer_final, encoded_layer4_final)\n",
        "\n",
        "# Freeze the final autoencoder Encoder layers\n",
        "encoder_final.trainable = False\n",
        "\n",
        "# Create the pre-trained model for the final autoencoder\n",
        "pretrained_model_final = Model(input_layer_final, encoder_final(input_layer_final))\n",
        "\n",
        "# Extract the hidden layer output of the final autoencoder\n",
        "hidden_layer_output_final = encoder_final.predict(x_train1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxmW0uaT_FLC"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['mean_absolute_error'], label='mean_absolute_error')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation_mean_absolute_error')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 0.35])\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlZ6GXCxNkxu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}